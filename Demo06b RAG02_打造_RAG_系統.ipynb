{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yenlung/AI-Demo/blob/master/%E3%80%90Demo06a%E3%80%91RAG01_%E6%89%93%E9%80%A0%E5%90%91%E9%87%8F%E8%B3%87%E6%96%99%E5%BA%AB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiaA78w0M3Io"
      },
      "source": [
        "To Do list\n",
        "\n",
        "- 透過 TrendMicro 的威脅情資報告與使用者端的情境建立 Embedding，讓 Chatbot 參考威脅情資對使用者的處境進行分析。並給出可能的攻擊面向或是應急處理方式。\n",
        "  - 威脅情資的 Embedding 處理\n",
        "  - Chatbot Prompting Stack\n",
        "  - FrontEnd Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build Vector Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWHn5kWsoJFT"
      },
      "outputs": [],
      "source": [
        "# Embedding\n",
        "import pandas\n",
        "import tqdm as notebook_tqdm\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document \n",
        "# Origin is \"from langchain_community.document_loaders import TextLoader, PyPDFLoader, UnstructuredWordDocumentLoader\", but i want to translate data via json. \n",
        "\n",
        "class ThreatIntelligenceEmbeddingModel() :\n",
        "    def __init__(self, \n",
        "                 threat_intelligence_data_path : str, \n",
        "                 embedding_model : str,\n",
        "                 chunk_size : int,\n",
        "                 chunk_overlap : int,\n",
        "                 vectorstore_path : str):\n",
        "        # data \n",
        "        self.threat_intelligence_data_path = threat_intelligence_data_path\n",
        "        self.threat_intelligence_embedding_db = \"none\" \n",
        "        self.vectorstore_path = vectorstore_path\n",
        "\n",
        "        # embedding parameter\n",
        "        self.embedding_model = embedding_model # \"intfloat/multilingual-e5-small\" \n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.pre_embedding_texts = []\n",
        "        self.split_pre_embedding_texts = []\n",
        "        self.vectorstore = \"none\"\n",
        "\n",
        "    def embedding_execute(self) : \n",
        "        df = pandas.read_json(self.threat_intelligence_data_path)\n",
        "        df = df.fillna(\"none\")\n",
        "        for index, row in df.iterrows() : \n",
        "            post_tile = row[\"title\"]\n",
        "            post_content = row[\"content\"]\n",
        "            pre_embedding_text = f\"Title : {post_tile} \\n Content : {post_content[0:300]}\" # save money\n",
        "            self.pre_embedding_texts.append(Document(page_content= pre_embedding_text))\n",
        "        \n",
        "        splitter = RecursiveCharacterTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
        "        self.split_pre_embedding_texts = splitter.split_documents(self.pre_embedding_texts)\n",
        "        self.vectorstore = FAISS.from_documents(self.split_pre_embedding_texts, HuggingFaceEmbeddings(model_name= self.embedding_model))\n",
        "    \n",
        "    def vector_store(self) : \n",
        "        self.vectorstore.save_local(self.vectorstore_path + \"/faiss_db\")\n",
        "\n",
        "if __name__ == \"__main__\" : \n",
        "    embedding_process = ThreatIntelligenceEmbeddingModel(\n",
        "                            threat_intelligence_data_path = \"data/cybersecurity_intelligence.json\",\n",
        "                            embedding_model = \"intfloat/multilingual-e5-small\",\n",
        "                            chunk_size = 500,\n",
        "                            chunk_overlap = 100,\n",
        "                            vectorstore_path = \"data\"\n",
        "    )\n",
        "    embedding_process.embedding_execute()\n",
        "    embedding_process.vector_store()\n",
        "    print(\"Emedding Completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ChatBot Stack "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "client = OpenAI(\n",
        "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        ")\n",
        "\n",
        "# cot prompt + RAG\n",
        "system_prompt_templete = '''\n",
        "    你是一個優秀的資安分析師，現在需要幫助使用者分辨當前的情境屬於何種資安威脅。並參考威脅情資內容後，根據以下內容 Step by Step 分析。\n",
        "    參考情資與使用者情境分析，\n",
        "    1. 使用者可能是如何被入侵的\n",
        "    2. 使用者目前可以先做何種緩解措施\n",
        "'''\n",
        "\n",
        "# RAG Post\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-small\")\n",
        "vectorstore = FAISS.load_local(\n",
        "    \"data/faiss_db\",\n",
        "    embeddings=embedding_model,\n",
        "    allow_dangerous_deserialization=True\n",
        ")\n",
        "\n",
        "messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt_templete}\n",
        "            ]\n",
        "\n",
        "import gradio\n",
        "with gradio.Blocks() as demo:\n",
        "    gradio.Markdown(\"# Professional Security Consulting\")\n",
        "    chatbot = gradio.Chatbot(type=\"messages\")\n",
        "    msg = gradio.Textbox(placeholder=\"請輸入你的問題...\")\n",
        "    state = gradio.State(messages)\n",
        "\n",
        "    def main_chatbot(user_prompt, messages):\n",
        "        results = vectorstore.similarity_search(user_prompt, k=1)\n",
        "        RAG_Post = results[0].page_content\n",
        "        user_prompt_templete = '''\n",
        "            使用者情境\n",
        "                {user_prompt}\n",
        "            威脅情資  \n",
        "                {RAG_Post}\n",
        "        '''\n",
        "\n",
        "        messages.append({\"role\": \"user\", \"content\": user_prompt_templete.format(user_prompt = user_prompt, RAG_Post= RAG_Post)})\n",
        "\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            model= \"gpt-4o-mini\", # save money\n",
        "            messages= messages,\n",
        "            max_tokens= 500 # cot very expensive must be limit output token.\n",
        "        )\n",
        "\n",
        "        reply = chat_completion.choices[0].message.content\n",
        "        messages.append({\"role\": \"assistant\", \"content\": reply}) # 透過添加歷史對話紀錄，變相讓 LLM 記得說了些什麼。改成這個寫法之後要 debug 也會比較方便。\n",
        "\n",
        "        return \"\", messages, messages\n",
        "\n",
        "    msg.submit(\n",
        "            fn=main_chatbot,\n",
        "            inputs=[msg, state],\n",
        "            outputs=[msg, chatbot, state]\n",
        "        )\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_prompt = '''\n",
        "    My desktop is showing a 24-hour countdown, demanding that I pay in Bitcoin to unlock it, or else my data will be stolen.\n",
        "\n",
        "    I just discovered a strange folder placed in the root directory of my Windows system. It's named \"ysytem32\n",
        "    '''\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
