{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6e9d19",
   "metadata": {},
   "source": [
    "# To-do List\n",
    "# Abstract\n",
    "According to the course requirements, we are tasked with building an LLM application based on the Reflection Module.\n",
    "\n",
    "Therefore, in this assignment, I will simulate a real-world scenario to enhance a RAG-based Threat Intelligence Security Chatbot using the Reflection Module.\n",
    "\n",
    "# 1. Introduction\n",
    "## 1.1. Reflection Module\n",
    "The Reflection Module consists of three key stages:\n",
    "\n",
    "Firstly, it processes the user's input using an LLM.\n",
    "\n",
    "Secondly, it uses the LLM again to refine the initial output.\n",
    "\n",
    "Finally, it generates the improved result.\n",
    "\n",
    "## 1.2. Enhancing RAG Applications with the Reflection Module\n",
    "In cybersecurity scenariosâ€”whether in threat intelligence reports or online articlesâ€”researchers often use highly technical or obscure terminology. This makes it difficult to directly map a user's input to relevant documents in a Threat Intelligence Database using vector search alone, often leading to misinterpretations.\n",
    "\n",
    "To address this, we propose using the Reflection Module to pre-process and refine user input. By transforming non-expert language into more professional and domain-specific terms, we aim to improve the relevance and accuracy of vector search results.\n",
    "\n",
    "# 2. Development\n",
    "Based on the aforementioned considerations,\n",
    "In this assignment for the course \"AI Agent: Reflection Module\", I will develop a security chatbot with the following features:\n",
    "\n",
    "Reflection Module-enhanced Chatbot\n",
    "\n",
    "Threat Intelligence-aware Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82512f6",
   "metadata": {},
   "source": [
    "# Reflection Module Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a4f2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "reflection_client = OpenAI(\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "\n",
    "def reply(system_prompt,\n",
    "          user_prompt\n",
    "          ):\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    chat_completion = reflection_client.chat.completions.create(\n",
    "        model= \"gpt-4o-mini\", # save money\n",
    "        messages= messages,\n",
    "        max_tokens= 500 # cot very expensive must be limit output token.\n",
    "    )\n",
    "\n",
    "\n",
    "    reply = chat_completion.choices[0].message.content\n",
    "\n",
    "    return reply\n",
    "\n",
    "def reflect_post(user_prompt):\n",
    "    # Step 1: Writer åˆç¨¿\n",
    "    system_writer = \"ä½ æ˜¯ä¸€å€‹å„ªç§€çš„è³‡å®‰åˆ†æé¡§å•ï¼Œè«‹å¹«åŠ©ä½¿ç”¨è€…æ¢³ç†æƒ…å¢ƒï¼Œå°‡ä½¿ç”¨è€…è¼ƒä¸å°ˆæ¥­çš„è¨€èªè½‰æ›ç‚ºé©åˆç”¨æ–¼å¨è„…æƒ…è³‡çš„ç‰ˆæœ¬\"\n",
    "    first_version = reply(system_prompt= system_writer, \n",
    "                          user_prompt = user_prompt\n",
    "                          )\n",
    "\n",
    "\n",
    "    # Step 2: Reviewer çµ¦å»ºè­°\n",
    "    system_reviewer = \"ä½ æ˜¯ä¸€ä½è³‡å®‰æ–‡æ¡ˆæ½¤ç¨¿å°ˆå®¶ï¼Œæ“…é•·åˆ†æä¸€ç¯‡å¨è„…æƒ…è³‡æ˜¯å¦è¶³å¤ å°ˆæ¥­ï¼Œä¸¦çµ¦å‡ºå»ºè­°ã€‚\"\n",
    "    suggestion = reply(system_prompt= system_reviewer, \n",
    "                        user_prompt = first_version\n",
    "                        )\n",
    "\n",
    "\n",
    "    # Step 3: Writer å†å¯«ä¸€æ¬¡ï¼ˆæ ¹æ“šå»ºè­°ï¼‰\n",
    "    second_system_prompt = \"ä½ æ˜¯å€‹å„ªåŒ–çš„è³‡å®‰é¡§å•ï¼Œå–„æ–¼æ ¹æ“šä¿®æ”¹æ„è¦‹å„ªåŒ–å¨è„…æƒ…è³‡çš„çµæ§‹\"\n",
    "    second_prompt = f\"é€™æ˜¯æˆ‘å‰›å‰›å¯«çš„è²¼æ–‡ï¼š\\n{first_version}\\n\\né€™æ˜¯ä¿®æ”¹å»ºè­°ï¼š\\n{suggestion}\\n\\nè«‹æ ¹æ“šé€™äº›å»ºè­°ï¼Œå¹«æˆ‘æ”¹å¾—æ›´ç”Ÿæ´»åŒ–ã€æ›´è‡ªç„¶ã€‚è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡, ä¸¦ä¸”åªè¦è¼¸å‡ºæ”¹å¥½çš„æ–‡ç« å°±å¯ä»¥äº†ã€‚\"\n",
    "\n",
    "    second_version = reply(system_prompt= second_system_prompt, \n",
    "                            user_prompt = second_prompt\n",
    "                            )\n",
    "    \n",
    "    return first_version, suggestion, second_version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "331a5a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio\n",
    "with gradio.Blocks() as demo:\n",
    "    gradio.Markdown(\"### ğŸ¤– è³‡å®‰å¨è„…æè¿°å„ªåŒ–ï¼ˆReflection Agentï¼‰\")\n",
    "    user_input = gradio.Textbox(label=\"è«‹è¼¸å…¥ä½ ä»Šå¤©ç™¼ç¾çš„è³‡å®‰äº‹ä»¶\")\n",
    "    btn = gradio.Button(\"ç”Ÿæˆè²¼æ–‡ & ä¿®æ­£å»ºè­°\")\n",
    "\n",
    "    with gradio.Row():\n",
    "        out1 = gradio.Textbox(label=\"ğŸŒŸ ç¬¬ä¸€ç‰ˆè²¼æ–‡ (model_writer)\")\n",
    "        out2 = gradio.Textbox(label=\"ğŸ§ ä¿®æ”¹å»ºè­° (model_reviewer)\")\n",
    "        out3 = gradio.Textbox(label=\"âœ¨ ç¬¬äºŒç‰ˆè²¼æ–‡ (model_writer æ”¹å¯«)\")\n",
    "\n",
    "    btn.click(reflect_post, inputs=[user_input], outputs=[out1, out2, out3])\n",
    "\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c13a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = '''\n",
    "    My desktop is showing a 24-hour countdown, demanding that I pay in Bitcoin to unlock it, or else my data will be stolen.\n",
    "\n",
    "    I just discovered a strange folder placed in the root directory of my Windows system. It's named \"ysytem32\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d04b1",
   "metadata": {},
   "source": [
    "# Threat Intellgence Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd58804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "import pandas\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document \n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# cot prompt + RAG\n",
    "system_prompt_templete = '''\n",
    "    ä½ æ˜¯ä¸€å€‹å„ªç§€çš„è³‡å®‰åˆ†æå¸«ï¼Œç¾åœ¨éœ€è¦å¹«åŠ©ä½¿ç”¨è€…åˆ†è¾¨ç•¶å‰çš„æƒ…å¢ƒå±¬æ–¼ä½•ç¨®è³‡å®‰å¨è„…ã€‚ä¸¦åƒè€ƒå¨è„…æƒ…è³‡å…§å®¹å¾Œï¼Œæ ¹æ“šä»¥ä¸‹å…§å®¹ Step by Step åˆ†æã€‚\n",
    "    åƒè€ƒæƒ…è³‡èˆ‡ä½¿ç”¨è€…æƒ…å¢ƒåˆ†æï¼Œ\n",
    "    1. ä½¿ç”¨è€…å¯èƒ½æ˜¯å¦‚ä½•è¢«å…¥ä¾µçš„\n",
    "    2. ä½¿ç”¨è€…ç›®å‰å¯ä»¥å…ˆåšä½•ç¨®ç·©è§£æªæ–½\n",
    "'''\n",
    "\n",
    "# RAG Post\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-small\")\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"data/faiss_db\",\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt_templete}\n",
    "            ]\n",
    "\n",
    "import gradio\n",
    "with gradio.Blocks() as demo:\n",
    "    gradio.Markdown(\"# Professional Security Consulting\")\n",
    "    chatbot = gradio.Chatbot(type=\"messages\")\n",
    "    msg = gradio.Textbox(placeholder=\"è«‹è¼¸å…¥ä½ çš„å•é¡Œ...\")\n",
    "    state = gradio.State(messages)\n",
    "\n",
    "    def main_chatbot(user_prompt, messages):\n",
    "        results = vectorstore.similarity_search(user_prompt, k=1)\n",
    "        RAG_Post = results[0].page_content\n",
    "        user_prompt_templete = '''\n",
    "            ä½¿ç”¨è€…æƒ…å¢ƒ\n",
    "                {user_prompt}\n",
    "            å¨è„…æƒ…è³‡  \n",
    "                {RAG_Post}\n",
    "        '''\n",
    "\n",
    "        messages.append({\"role\": \"user\", \"content\": user_prompt_templete.format(user_prompt = user_prompt, RAG_Post= RAG_Post)})\n",
    "\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model= \"gpt-4o-mini\", # save money\n",
    "            messages= messages,\n",
    "            max_tokens= 500 # cot very expensive must be limit output token.\n",
    "        )\n",
    "\n",
    "        reply = chat_completion.choices[0].message.content\n",
    "        messages.append({\"role\": \"assistant\", \"content\": reply}) # é€éæ·»åŠ æ­·å²å°è©±ç´€éŒ„ï¼Œè®Šç›¸è®“ LLM è¨˜å¾—èªªäº†äº›ä»€éº¼ã€‚æ”¹æˆé€™å€‹å¯«æ³•ä¹‹å¾Œè¦ debug ä¹Ÿæœƒæ¯”è¼ƒæ–¹ä¾¿ã€‚\n",
    "\n",
    "        return \"\", messages, messages\n",
    "\n",
    "    msg.submit(\n",
    "            fn=main_chatbot,\n",
    "            inputs=[msg, state],\n",
    "            outputs=[msg, chatbot, state]\n",
    "        )\n",
    "\n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf2f712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
